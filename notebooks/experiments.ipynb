{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8f93e2bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f93e2bf",
        "outputId": "00a665b2-4c10-487a-93ed-9d7c15cd43e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\u2713 GPU: Tesla T4\n",
            "\u2713 GPU Memory: 15.83 GB\n",
            "\u2713 Mixed precision training enabled\n"
          ]
        }
      ],
      "source": [
        "# Setup and Imports\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertModel, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path.cwd().parent / 'src'))\n",
        "from multitask_bert import MultiTaskBERT\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Verify GPU usage\n",
        "if device.type == 'cuda':\n",
        "    print(f\"\u2713 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"\u2713 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f WARNING: Not using GPU! Training will be very slow.\")\n",
        "    print(\"In Colab: Runtime \u2192 Change runtime type \u2192 Hardware accelerator \u2192 GPU\")\n",
        "\n",
        "# Enable mixed precision training\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "use_amp = device.type == 'cuda'\n",
        "scaler = GradScaler() if use_amp else None\n",
        "if use_amp:\n",
        "    print(\"\u2713 Mixed precision training enabled\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "A1MH-rCEsDZU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1MH-rCEsDZU",
        "outputId": "c5437a46-1456-4e21-9003-00f016e57028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_PATH = 'drive/MyDrive/Deep Learning project/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e695eb",
      "metadata": {
        "id": "d6e695eb"
      },
      "source": [
        "# Part 1.3 \u2014 Experiments\n",
        "\n",
        "This notebook contains all experimental configurations for toxicity detection:\n",
        "1. **Baseline**: Fine-tune BERT only on toxicity (single-task)\n",
        "2. **Multi-Task**: Train jointly on both tasks (shared encoder)\n",
        "3. **Sequential**: Pre-train on emotion \u2192 fine-tune for toxicity\n",
        "4. **Ablation studies** with different loss weights (1:1, 2:1, 1:2)\n",
        "\n",
        "### Metrics Tracked\n",
        "- **F1 Score**: Primary metric for toxicity classification\n",
        "- **Precision**: Precision for toxic class\n",
        "- **Recall**: Recall for toxic class\n",
        "- **ROC-AUC**: Area under ROC curve\n",
        "- **Accuracy**: Overall classification accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989bdaca",
      "metadata": {
        "id": "989bdaca"
      },
      "outputs": [],
      "source": [
        "class ToxicityDataset(Dataset):\n",
        "    \"\"\"Dataset for toxicity classification.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.tokens = data[\"tokens\"]\n",
        "        self.labels = data[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.tokens[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.tokens[\"attention_mask\"][idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    \"\"\"Dataset for emotion classification.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.tokens = data[\"tokens\"]\n",
        "        self.labels = data[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.tokens[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.tokens[\"attention_mask\"][idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "def toxicity_collate_fn(batch):\n",
        "    \"\"\"Collate function for toxicity batches.\"\"\"\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
        "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
        "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
        "    }\n",
        "\n",
        "def emotion_collate_fn(batch):\n",
        "    \"\"\"Collate function for emotion batches.\"\"\"\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
        "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
        "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
        "    }\n",
        "\n",
        "def load_data(data_root=\"../data/processed/tokenized\"):\n",
        "    \"\"\"Load separate datasets for toxicity and emotion classification.\"\"\"\n",
        "    data_root = Path(data_root)\n",
        "\n",
        "    tox_dir = data_root / \"toxicity\"\n",
        "    emo_dir = data_root / \"emotion\"\n",
        "\n",
        "    if not tox_dir.exists() or not emo_dir.exists():\n",
        "        raise FileNotFoundError(f\"Data directories not found: {tox_dir}, {emo_dir}\")\n",
        "\n",
        "    print(\"Loading toxicity data...\")\n",
        "    toxicity_train_data = torch.load(tox_dir / \"train.pt\")\n",
        "    toxicity_val_data = torch.load(tox_dir / \"val.pt\")\n",
        "    toxicity_test_data = torch.load(tox_dir / \"test.pt\")\n",
        "\n",
        "    print(\"Loading emotion data...\")\n",
        "    emotion_train_data = torch.load(emo_dir / \"train.pt\")\n",
        "    emotion_val_data = torch.load(emo_dir / \"val.pt\")\n",
        "    emotion_test_data = torch.load(emo_dir / \"test.pt\")\n",
        "\n",
        "    tox_train = ToxicityDataset(toxicity_train_data)\n",
        "    tox_val = ToxicityDataset(toxicity_val_data)\n",
        "    tox_test = ToxicityDataset(toxicity_test_data)\n",
        "    emo_train = EmotionDataset(emotion_train_data)\n",
        "    emo_val = EmotionDataset(emotion_val_data)\n",
        "    emo_test = EmotionDataset(emotion_test_data)\n",
        "\n",
        "    print(f\"Toxicity - Train: {len(tox_train)}, Val: {len(tox_val)}, Test: {len(tox_test)}\")\n",
        "    print(f\"Emotion - Train: {len(emo_train)}, Val: {len(emo_val)}, Test: {len(emo_test)}\")\n",
        "\n",
        "    return tox_train, tox_val, tox_test, emo_train, emo_val, emo_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "38a1fb45",
      "metadata": {
        "id": "38a1fb45"
      },
      "outputs": [],
      "source": [
        "def evaluate_toxicity(model, dataloader, device):\n",
        "    \"\"\"Evaluate toxicity model and return metrics.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].long().to(device)\n",
        "\n",
        "            # Get predictions (only toxicity head)\n",
        "            if isinstance(model, MultiTaskBERT):\n",
        "                logits, _ = model(input_ids, attention_mask)\n",
        "            else:\n",
        "                # Single-task baseline model\n",
        "                logits = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # ROC-AUC (multi-class)\n",
        "    try:\n",
        "        # Convert to one-hot for multi-class ROC-AUC\n",
        "        num_classes = len(np.unique(all_labels))\n",
        "        if num_classes > 2:\n",
        "            # Use macro average for multi-class\n",
        "            roc_auc = roc_auc_score(all_labels, all_preds, average='macro', multi_class='ovr', labels=range(num_classes))\n",
        "        else:\n",
        "            roc_auc = roc_auc_score(all_labels, all_preds)\n",
        "    except:\n",
        "        roc_auc = 0.0\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    return {\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'roc_auc': roc_auc,\n",
        "        'accuracy': accuracy,\n",
        "        'loss': avg_loss\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45b16e27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45b16e27",
        "outputId": "4045a6b9-cc2e-4bd7-86d8-db636e1dac8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Epochs: 3\n",
            "  Batch size: 32\n",
            "  Learning rate: 2e-05\n",
            "Loading toxicity data...\n",
            "Loading emotion data...\n",
            "Toxicity - Train: 126580, Val: 15823, Test: 15823\n",
            "Emotion - Train: 39064, Val: 4883, Test: 4883\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "DATA_BASE_PATH = '../data/processed/tokenized'\n",
        "TOX_DATA_PATH = f'{DATA_BASE_PATH}/toxicity'\n",
        "EMO_DATA_PATH = f'{DATA_BASE_PATH}/emotion'\n",
        "\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "GRADIENT_CLIP = 1.0\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Load data\n",
        "tox_train, tox_val, tox_test, emo_train, emo_val, emo_test = load_data(DATA_BASE_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d11a4f9d",
      "metadata": {
        "id": "d11a4f9d"
      },
      "source": [
        "### 1. Baseline Model: Single-Task Toxicity Fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "32fcf97f",
      "metadata": {
        "id": "32fcf97f"
      },
      "outputs": [],
      "source": [
        "# Baseline: Single-task BERT for toxicity only\n",
        "class BaselineBERT(nn.Module):\n",
        "    \"\"\"Single-task BERT model for toxicity classification only.\"\"\"\n",
        "\n",
        "    def __init__(self, bert_model_name='bert-base-uncased', num_labels=6):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "def train_baseline(model, train_loader, val_loader, epochs=3, lr=2e-5, device='cpu', save_path='checkpoints/baseline_best.pt', use_amp=False, scaler=None):\n",
        "    \"\"\"Train baseline single-task model.\"\"\"\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    steps_per_epoch = len(train_loader)\n",
        "    total_steps = steps_per_epoch * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_f1 = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].long().to(device)\n",
        "\n",
        "            # Mixed precision training\n",
        "            if use_amp and scaler is not None:\n",
        "                with autocast():\n",
        "                    logits = model(input_ids, attention_mask)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                logits = model(input_ids, attention_mask)\n",
        "                loss = criterion(logits, labels)\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
        "                optimizer.step()\n",
        "\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        val_metrics = evaluate_toxicity(model, val_loader, device)\n",
        "        print(f\"Val Metrics - F1: {val_metrics['f1']:.4f}, Precision: {val_metrics['precision']:.4f}, \"\n",
        "              f\"Recall: {val_metrics['recall']:.4f}, ROC-AUC: {val_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_metrics['f1'] > best_val_f1:\n",
        "            best_val_f1 = val_metrics['f1']\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_f1': val_metrics['f1'],\n",
        "                'val_metrics': val_metrics\n",
        "            }, save_path)\n",
        "            print(f\"Saved best model (F1: {val_metrics['f1']:.4f})\")\n",
        "\n",
        "    return model, val_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b4cc5223",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "b4cc5223",
        "outputId": "7924b07c-85c8-40dc-ce31-905c27422e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Training Baseline Model (Single-Task Toxicity)\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a02261ed3f26445b9924944c1f980093",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b4ed8f010424e1e9121f77c5b90eda9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3956/3956 [13:06<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0740\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 495/495 [01:45<00:00,  4.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Metrics - F1: 0.9937, Precision: 0.9915, Recall: 0.9958, ROC-AUC: 0.0000\n",
            "Saved best model (F1: 0.9937)\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3956/3956 [13:08<00:00,  5.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 495/495 [01:45<00:00,  4.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Metrics - F1: 0.9937, Precision: 0.9915, Recall: 0.9958, ROC-AUC: 0.0000\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3956/3956 [13:04<00:00,  5.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0213\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 495/495 [01:45<00:00,  4.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Metrics - F1: 0.9935, Precision: 0.9922, Recall: 0.9953, ROC-AUC: 0.0000\n",
            "\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 495/495 [01:45<00:00,  4.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Metrics - F1: 0.9919, Precision: 0.9903, Recall: 0.9941, ROC-AUC: 0.0000\n",
            "Saved final baseline model\n"
          ]
        }
      ],
      "source": [
        "# Train baseline\n",
        "print(\"=\"*80)\n",
        "print(\"Training Baseline Model (Single-Task Toxicity)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "baseline_model = BaselineBERT()\n",
        "tox_train_loader = DataLoader(tox_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=toxicity_collate_fn, num_workers=2, pin_memory=True)\n",
        "tox_val_loader = DataLoader(tox_val, batch_size=BATCH_SIZE, collate_fn=toxicity_collate_fn, num_workers=2, pin_memory=True)\n",
        "tox_test_loader = DataLoader(tox_test, batch_size=BATCH_SIZE, collate_fn=toxicity_collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "baseline_model, baseline_val_metrics = train_baseline(\n",
        "    baseline_model, tox_train_loader, tox_val_loader,\n",
        "    epochs=EPOCHS, lr=LEARNING_RATE, device=device,\n",
        "    save_path='checkpoints/baseline_best.pt', use_amp=use_amp, scaler=scaler\n",
        ")\n",
        "\n",
        "# Test evaluation\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "baseline_test_metrics = evaluate_toxicity(baseline_model, tox_test_loader, device)\n",
        "print(f\"Test Metrics - F1: {baseline_test_metrics['f1']:.4f}, Precision: {baseline_test_metrics['precision']:.4f}, \"\n",
        "      f\"Recall: {baseline_test_metrics['recall']:.4f}, ROC-AUC: {baseline_test_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(baseline_model.state_dict(), 'checkpoints/baseline_final.pt')\n",
        "print(\"Saved final baseline model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "421f84ba",
      "metadata": {
        "id": "421f84ba"
      },
      "source": [
        "### 2. Multi-Task Model: Joint Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "82f8c0a2",
      "metadata": {
        "id": "82f8c0a2"
      },
      "outputs": [],
      "source": [
        "def train_multitask(model, tox_train_loader, tox_val_loader, emo_train_loader, emo_val_loader,\n",
        "                    epochs=3, lr=2e-5, device='cpu', save_path='checkpoints/multitask_best.pt'):\n",
        "    \"\"\"Train multi-task model with alternating batches.\"\"\"\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    steps_per_epoch = max(len(tox_train_loader), len(emo_train_loader))\n",
        "    total_steps = steps_per_epoch * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    criterion_tox = nn.CrossEntropyLoss()\n",
        "    criterion_emo = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_f1 = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_tox_loss = 0\n",
        "        total_emo_loss = 0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        tox_iter = iter(tox_train_loader)\n",
        "        emo_iter = iter(emo_train_loader)\n",
        "\n",
        "        progress_bar = tqdm(range(steps_per_epoch), desc=\"Training\")\n",
        "        for step in progress_bar:\n",
        "            # Train on toxicity\n",
        "            try:\n",
        "                tox_batch = next(tox_iter)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                input_ids = tox_batch[\"input_ids\"].to(device)\n",
        "                attention_mask = tox_batch[\"attention_mask\"].to(device)\n",
        "                tox_labels = tox_batch[\"labels\"].long().to(device)\n",
        "\n",
        "                tox_logits, _ = model(input_ids, attention_mask)\n",
        "                loss = model.lambda_tox * criterion_tox(tox_logits, tox_labels)\n",
        "\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_tox_loss += loss.item()\n",
        "            except StopIteration:\n",
        "                tox_iter = iter(tox_train_loader)\n",
        "\n",
        "            # Train on emotion\n",
        "            try:\n",
        "                emo_batch = next(emo_iter)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                input_ids = emo_batch[\"input_ids\"].to(device)\n",
        "                attention_mask = emo_batch[\"attention_mask\"].to(device)\n",
        "                emo_labels = emo_batch[\"labels\"].float().to(device)\n",
        "\n",
        "                _, emo_logits = model(input_ids, attention_mask)\n",
        "                loss = model.lambda_emo * criterion_emo(emo_logits, emo_labels)\n",
        "\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_emo_loss += loss.item()\n",
        "            except StopIteration:\n",
        "                emo_iter = iter(emo_train_loader)\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'tox_loss': f'{total_tox_loss/(step+1):.4f}',\n",
        "                'emo_loss': f'{total_emo_loss/(step+1):.4f}'\n",
        "            })\n",
        "\n",
        "        avg_tox_loss = total_tox_loss / steps_per_epoch\n",
        "        avg_emo_loss = total_emo_loss / steps_per_epoch\n",
        "        print(f\"Train Loss - Tox: {avg_tox_loss:.4f}, Emo: {avg_emo_loss:.4f}\")\n",
        "\n",
        "        # Validation (only toxicity metrics)\n",
        "        val_metrics = evaluate_toxicity(model, tox_val_loader, device)\n",
        "        print(f\"Val Metrics - F1: {val_metrics['f1']:.4f}, Precision: {val_metrics['precision']:.4f}, \"\n",
        "              f\"Recall: {val_metrics['recall']:.4f}, ROC-AUC: {val_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_metrics['f1'] > best_val_f1:\n",
        "            best_val_f1 = val_metrics['f1']\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_f1': val_metrics['f1'],\n",
        "                'val_metrics': val_metrics,\n",
        "                'lambda_tox': model.lambda_tox,\n",
        "                'lambda_emo': model.lambda_emo\n",
        "            }, save_path)\n",
        "            print(f\"Saved best model (F1: {val_metrics['f1']:.4f})\")\n",
        "\n",
        "    return model, val_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ecac3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ecac3d",
        "outputId": "7d9a99e5-5702-4129-a343-35239e85c5eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Training Multi-Task Models with Different Loss Weights\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Multi-Task Training: \u03bb_tox=1.0, \u03bb_emo=1.0\n",
            "================================================================================\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   6%|\u258c         | 247/3956 [05:14<1:18:10,  1.26s/it, tox_loss=0.4848, emo_loss=0.3900]"
          ]
        }
      ],
      "source": [
        "# Train multi-task with different loss weights\n",
        "print(\"=\"*80)\n",
        "print(\"Training Multi-Task Models with Different Loss Weights\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "emo_train_loader = DataLoader(emo_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=emotion_collate_fn, num_workers=0)\n",
        "emo_val_loader = DataLoader(emo_val, batch_size=BATCH_SIZE, collate_fn=emotion_collate_fn, num_workers=0)\n",
        "\n",
        "# Loss weight configurations\n",
        "loss_configs = [\n",
        "    (1.0, 1.0, '1_1'),\n",
        "    (2.0, 1.0, '2_1'),\n",
        "    (1.0, 2.0, '1_2')\n",
        "]\n",
        "\n",
        "multitask_results = []\n",
        "\n",
        "for lambda_tox, lambda_emo, suffix in loss_configs:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Multi-Task Training: \u03bb_tox={lambda_tox}, \u03bb_emo={lambda_emo}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    model = MultiTaskBERT(lambda_tox=lambda_tox, lambda_emo=lambda_emo)\n",
        "    save_path = f'checkpoints/multitask_w{lambda_tox}_{lambda_emo}_best.pt'\n",
        "\n",
        "    model, val_metrics = train_multitask(\n",
        "        model, tox_train_loader, tox_val_loader, emo_train_loader, emo_val_loader,\n",
        "        epochs=EPOCHS, lr=LEARNING_RATE, device=device, save_path=save_path\n",
        "    )\n",
        "\n",
        "    # Test evaluation\n",
        "    test_metrics = evaluate_toxicity(model, tox_test_loader, device)\n",
        "    print(f\"\\nTest Metrics - F1: {test_metrics['f1']:.4f}, Precision: {test_metrics['precision']:.4f}, \"\n",
        "          f\"Recall: {test_metrics['recall']:.4f}, ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), f'checkpoints/multitask_w{lambda_tox}_{lambda_emo}_final.pt')\n",
        "\n",
        "    multitask_results.append({\n",
        "        'model': 'multitask',\n",
        "        'config': f'multitask_w{lambda_tox}_{lambda_emo}',\n",
        "        'lambda_tox': lambda_tox,\n",
        "        'lambda_emo': lambda_emo,\n",
        "        'val_f1': val_metrics['f1'],\n",
        "        'val_precision': val_metrics['precision'],\n",
        "        'val_recall': val_metrics['recall'],\n",
        "        'val_roc_auc': val_metrics['roc_auc'],\n",
        "        'val_accuracy': val_metrics['accuracy'],\n",
        "        'test_f1': test_metrics['f1'],\n",
        "        'test_precision': test_metrics['precision'],\n",
        "        'test_recall': test_metrics['recall'],\n",
        "        'test_roc_auc': test_metrics['roc_auc'],\n",
        "        'test_accuracy': test_metrics['accuracy']\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b31400",
      "metadata": {
        "id": "c1b31400"
      },
      "source": [
        "### 3. Sequential Model: Pre-train on Emotion \u2192 Fine-tune for Toxicity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cc04dd2",
      "metadata": {
        "id": "6cc04dd2"
      },
      "outputs": [],
      "source": [
        "def train_sequential(emo_train_loader, emo_val_loader, tox_train_loader, tox_val_loader,\n",
        "                     epochs_pretrain=2, epochs_finetune=3, lr=2e-5, device='cpu',\n",
        "                     save_path='checkpoints/sequential_best.pt'):\n",
        "    \"\"\"Sequential training: pre-train on emotion, then fine-tune for toxicity.\"\"\"\n",
        "\n",
        "    # Step 1: Pre-train on emotion\n",
        "    print(\"=\"*80)\n",
        "    print(\"Step 1: Pre-training on Emotion\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model = MultiTaskBERT(lambda_tox=0.0, lambda_emo=1.0)  # Only emotion loss\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    criterion_emo = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    steps_per_epoch = len(emo_train_loader)\n",
        "    total_steps = steps_per_epoch * epochs_pretrain\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs_pretrain):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        print(f\"\\nPre-train Epoch {epoch+1}/{epochs_pretrain}\")\n",
        "        for batch in tqdm(emo_train_loader, desc=\"Training\"):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            emo_labels = batch[\"labels\"].float().to(device)\n",
        "\n",
        "            _, emo_logits = model(input_ids, attention_mask)\n",
        "            loss = criterion_emo(emo_logits, emo_labels)\n",
        "\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(emo_train_loader)\n",
        "        print(f\"Pre-train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"\\nPre-training complete!\")\n",
        "\n",
        "    # Step 2: Fine-tune for toxicity\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Step 2: Fine-tuning for Toxicity\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Now switch to toxicity-only training\n",
        "    model.lambda_tox = 1.0\n",
        "    model.lambda_emo = 0.0  # Disable emotion loss\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr * 0.1)  # Lower learning rate for fine-tuning\n",
        "    criterion_tox = nn.CrossEntropyLoss()\n",
        "\n",
        "    steps_per_epoch = len(tox_train_loader)\n",
        "    total_steps = steps_per_epoch * epochs_finetune\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
        "\n",
        "    best_val_f1 = 0.0\n",
        "\n",
        "    for epoch in range(epochs_finetune):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        print(f\"\\nFine-tune Epoch {epoch+1}/{epochs_finetune}\")\n",
        "        for batch in tqdm(tox_train_loader, desc=\"Training\"):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            tox_labels = batch[\"labels\"].long().to(device)\n",
        "\n",
        "            tox_logits, _ = model(input_ids, attention_mask)\n",
        "            loss = criterion_tox(tox_logits, tox_labels)\n",
        "\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(tox_train_loader)\n",
        "        print(f\"Fine-tune Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        val_metrics = evaluate_toxicity(model, tox_val_loader, device)\n",
        "        print(f\"Val Metrics - F1: {val_metrics['f1']:.4f}, Precision: {val_metrics['precision']:.4f}, \"\n",
        "              f\"Recall: {val_metrics['recall']:.4f}, ROC-AUC: {val_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_metrics['f1'] > best_val_f1:\n",
        "            best_val_f1 = val_metrics['f1']\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_f1': val_metrics['f1'],\n",
        "                'val_metrics': val_metrics\n",
        "            }, save_path)\n",
        "            print(f\"Saved best model (F1: {val_metrics['f1']:.4f})\")\n",
        "\n",
        "    return model, val_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9d3999a",
      "metadata": {
        "id": "f9d3999a"
      },
      "outputs": [],
      "source": [
        "# Train sequential model\n",
        "sequential_model, sequential_val_metrics = train_sequential(\n",
        "    emo_train_loader, emo_val_loader, tox_train_loader, tox_val_loader,\n",
        "    epochs_pretrain=2, epochs_finetune=EPOCHS, lr=LEARNING_RATE, device=device,\n",
        "    save_path='checkpoints/sequential_best.pt'\n",
        ")\n",
        "\n",
        "# Test evaluation\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "sequential_test_metrics = evaluate_toxicity(sequential_model, tox_test_loader, device)\n",
        "print(f\"Test Metrics - F1: {sequential_test_metrics['f1']:.4f}, Precision: {sequential_test_metrics['precision']:.4f}, \"\n",
        "      f\"Recall: {sequential_test_metrics['recall']:.4f}, ROC-AUC: {sequential_test_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(sequential_model.state_dict(), 'checkpoints/sequential_final.pt')\n",
        "print(\"Saved final sequential model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1145d68",
      "metadata": {
        "id": "f1145d68"
      },
      "outputs": [],
      "source": [
        "# Compile all results\n",
        "all_results = []\n",
        "\n",
        "# Baseline results\n",
        "all_results.append({\n",
        "    'model': 'baseline',\n",
        "    'config': 'single_task',\n",
        "    'lambda_tox': 1.0,\n",
        "    'lambda_emo': 0.0,\n",
        "    'val_f1': baseline_val_metrics['f1'],\n",
        "    'val_precision': baseline_val_metrics['precision'],\n",
        "    'val_recall': baseline_val_metrics['recall'],\n",
        "    'val_roc_auc': baseline_val_metrics['roc_auc'],\n",
        "    'val_accuracy': baseline_val_metrics['accuracy'],\n",
        "    'test_f1': baseline_test_metrics['f1'],\n",
        "    'test_precision': baseline_test_metrics['precision'],\n",
        "    'test_recall': baseline_test_metrics['recall'],\n",
        "    'test_roc_auc': baseline_test_metrics['roc_auc'],\n",
        "    'test_accuracy': baseline_test_metrics['accuracy']\n",
        "})\n",
        "\n",
        "# Multi-task results\n",
        "for result in multitask_results:\n",
        "    all_results.append(result)\n",
        "\n",
        "# Sequential results\n",
        "all_results.append({\n",
        "    'model': 'sequential',\n",
        "    'config': 'pretrain_emotion_finetune_toxicity',\n",
        "    'lambda_tox': 1.0,\n",
        "    'lambda_emo': 1.0,\n",
        "    'val_f1': sequential_val_metrics['f1'],\n",
        "    'val_precision': sequential_val_metrics['precision'],\n",
        "    'val_recall': sequential_val_metrics['recall'],\n",
        "    'val_roc_auc': sequential_val_metrics['roc_auc'],\n",
        "    'val_accuracy': sequential_val_metrics['accuracy'],\n",
        "    'test_f1': sequential_test_metrics['f1'],\n",
        "    'test_precision': sequential_test_metrics['precision'],\n",
        "    'test_recall': sequential_test_metrics['recall'],\n",
        "    'test_roc_auc': sequential_test_metrics['roc_auc'],\n",
        "    'test_accuracy': sequential_test_metrics['accuracy']\n",
        "})\n",
        "\n",
        "# Create DataFrame and save\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_path = 'results/metrics.csv'\n",
        "results_df.to_csv(results_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"All Results Summary\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(f\"\\nResults saved to {results_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f13a895c",
      "metadata": {
        "id": "f13a895c"
      },
      "source": [
        "### Checkpoint Files:\n",
        "- `checkpoints/baseline_best.pt` / `baseline_final.pt`: Single-task toxicity model\n",
        "- `checkpoints/multitask_w1.0_1.0_best.pt` / `multitask_w1.0_1.0_final.pt`: Multi-task (1:1 weights)\n",
        "- `checkpoints/multitask_w2.0_1.0_best.pt` / `multitask_w2.0_1.0_final.pt`: Multi-task (2:1 weights)\n",
        "- `checkpoints/multitask_w1.0_2.0_best.pt` / `multitask_w1.0_2.0_final.pt`: Multi-task (1:2 weights)\n",
        "- `checkpoints/sequential_best.pt` / `sequential_final.pt`: Sequential training model\n",
        "\n",
        "### Metrics:\n",
        "All evaluation metrics (F1, Precision, Recall, ROC-AUC, Accuracy) are saved in `results/metrics.csv`\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}